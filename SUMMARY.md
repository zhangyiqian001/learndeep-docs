# Table of contents

* [Introduction](README.md)
* [1. 开山模型](chapter1/README.md)
  * [Playing Atari with Deep Reinforcement Learning](chapter1/article/Playing+Atari+with+Deep+Reinforcement+Learning+827583df-e794-463c-99f2-cf3f67fe9990.md)
  
  * [FCN:Fully Convolutional Networks for Semantic Segmentation](chapter1/article/FCN_Fully+Convolutional+Networks+for+Semantic+Segmentation+7ba6b006-838f-409c-916e-0699de48b3fc.md)
  
  * [U-Net:Convolutional Networks for Biomedical Image Segmentation](chapter1/article/U-Net_Convolutional+Networks+for+Biomedical+Image+Segmentation+abc51257-7672-432e-83ce-d920552d8550.md)
  
  * [GAN:Generative Adversarial Nets](chapter1/article/GAN_Generative+Adversarial+Nets+fc1b0774-6173-4f34-b424-dbab8a5d3af7.md)
  
  * [Attention Is All You Need](chapter1/article/Attention+Is+All+You+Need+5697d7af-2155-4eb4-9e3a-63d7727c1b17.md)
  
  * [GPT-1:Improving Language Understanding by Generative Pre-Training](chapter1/article/GPT-1_Improving+Language+Understanding+by+Generative+Pre-Training+e8311bc0-b181-4b42-91c5-6ec0eabf3bb1.md)
  
  * [InstructGPT:Training language models to follow instructions with human feedback](chapter1/article/InstructGPT_Training+language+models+to+follow+instructions+with+human+feedback+295ddc33-d3d8-401b-aaf6-e712f063609c.md)
  
  * [BERT:Pre-training of Deep Bidirectional Transformers for Language Understanding](chapter1/article/BERT_Pre-training+of+Deep+Bidirectional+Transformers+for+Language+Understanding+c63ffe59-577a-4a8e-812b-a306ea91f8d8.md)
  
  * [BART:Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](chapter1/article/BART_Denoising+Sequence-to-Sequence+Pre-training+for+Natural+Language+Generation,+Translation,+and+Comprehension+da7bcf69-e913-402c-9506-16221b816a95.md)
  
  * [T5:Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](chapter1/article/T5_Exploring+the+Limits+of+Transfer+Learning+with+a+Unified+Text-to-Text+Transformer+f135b4cd-6d37-4a2f-a579-90c5736378f0.md)
  
  * [ELMo:Deep contextualized word representations](chapter1/article/ELMo_Deep+contextualized+word+representations+28adb18f-986e-4525-bcda-35c5d9175dcd.md)
  
  * [ViT:AN IMAGE IS WORTH 16X16 WORDS_ TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE](chapter1/article/ViT_AN+IMAGE+IS+WORTH+16X16+WORDS_+TRANSFORMERS+FOR+IMAGE+RECOGNITION+AT+SCALE+4f160327-28cf-4943-9f52-b36de432337e.md)
  
  * [Distilling the Knowledge in a Neural Network](chapter1/article/Distilling+the+Knowledge+in+a+Neural+Network+a4d51fb7-586f-4bee-a293-f6bcfb41c843.md)
  
  * [DeiT:Training data-efficient image transformers & distillation through attention](chapter1/article/DeiT_Training+data-efficient+image+transformers+++distillation+through+attention+84d04d5c-2835-45c9-8545-9e007920f9ba.md)
  
  * [Swin Transformer:Hierarchical Vision Transformer using Shifted Windows](chapter1/article/Swin+Transformer_Hierarchical+Vision+Transformer+using+Shifted+Windows+9d8ba80e-b366-4464-9187-7de59aa85760.md)
  
  * [DETR:End-to-End Object Detection with Transformers](chapter1/article/DETR_End-to-End+Object+Detection+with+Transformers+bb4f3503-1f59-4e34-b261-3b83165e6e9e.md)
  
  * [CLIP:Learning Transferable Visual Models From Natural Language Supervision](chapter1/article/CLIP_Learning+Transferable+Visual+Models+From+Natural+Language+Supervision+9efc2900-5160-4a05-a624-68a35d26c23f.md)
  
  * [VAE:Auto-Encoding Variational Bayes](chapter1/article/VAE_Auto-Encoding+Variational+Bayes+ff3dd745-4822-4bb1-9c99-d931512df77b.md)
  
  * [VQ-VAE:Neural Discrete Representation Learning](chapter1/article/VQ-VAE_Neural+Discrete+Representation+Learning+207156ff-8f12-497e-a0f5-b11fdf2b3712.md)
  
  * [VQ-VAE2:Generating Diverse High-Fidelity Images with VQ-VAE-2](chapter1/article/VQ-VAE2_Generating+Diverse+High-Fidelity+Images+with+VQ-VAE-2+d8d21676-11d4-4836-98d2-9babbe65e784.md)
  
  * [KAN:Kolmogorov–Arnold Networks](chapter1/article/KAN_Kolmogorov%E2%80%93Arnold+Networks+a7e663f0-8c8c-4071-8630-eacaf17a50f4.md)
  
  * [Pixel RNN:Pixel Recurrent Neural Networks](chapter1/article/Pixel+RNN_Pixel+Recurrent+Neural+Networks+df0af303-e9af-4a29-a74d-2696b9a3c36e.md)
  
  * [Conditional Image Generation with PixelCNN Decoders](chapter1/article/Conditional+Image+Generation+with+PixelCNN+Decoders+607b537c-9732-4579-b29c-8812a7dbcef2.md)
  
  * [GQA:Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints](chapter1/article/GQA_Training+Generalized+Multi-Query+Transformer+Models+from+Multi-Head+Checkpoints+9c3cb968-6f34-4e84-8602-98246c47efe1.md)
  
  * [FlashAttention:Fast and Memory-Efficient Exact Attention with IO-Awareness](chapter1/article/FlashAttention_Fast+and+Memory-Efficient+Exact+Attention+with+IO-Awareness+93390a33-b714-4954-afed-1930b535c3a0.md)
  
  * [Efficient Memory Management for Large Language Model Serving with PagedAttention](chapter1/article/Efficient+Memory+Management+for+Large+Language+Model+Serving+with+PagedAttention+984c0dc0-250b-4618-abb2-41a85f68c588.md)
  
* [2. 自然语言处理](chapter2/README.md)
* [3. 计算机视觉](chapter3/README.md)
  * [DDPM:Denoising Diffusion Probabilistic Models](chapter3/article/DDPM_Denoising+Diffusion+Probabilistic+Models+4335257e-4570-43e8-bee2-0837d92150b6.md)

  * [DDIM:DENOISING DIFFUSION IMPLICIT MODELS](chapter3/article/DDIM_DENOISING+DIFFUSION+IMPLICIT+MODELS+2dc2998f-bd42-4b37-a8ca-da08243c3e7c.md)

  * [CDM:Diffusion Models Beat GANs on Image Synthesis](chapter3/article/CDM_Diffusion+Models+Beat+GANs+on+Image+Synthesis+96086eb1-ef19-49d4-83e9-83a4e9083fc7.md)

  * [SCORE-BASED GENERATIVE MODELING THROUGH STOCHASTIC DIFFERENTIAL EQUATIONS](chapter3/article/SCORE-BASED+GENERATIVE+MODELING+THROUGH+STOCHASTIC+DIFFERENTIAL+EQUATIONS+2d98ac84-84e8-466a-ac24-19ef5ed86706.md)

  * [CDM:CLASSIFIER-FREE DIFFUSION GUIDANCE](chapter3/article/CDM_CLASSIFIER-FREE+DIFFUSION+GUIDANCE+2cb68fba-87b0-4c7c-96bd-5cbe2e8c2080.md)

  * [RePaint:Inpainting using Denoising Diffusion Probabilistic Models](chapter3/article/RePaint_Inpainting+using+Denoising+Diffusion+Probabilistic+Models+06734663-1cc2-41eb-9cf1-2d15cf1c4dfe.md)

  * [Stable Diffusion:High-Resolution Image Synthesis with Latent Diffusion Models](chapter3/article/Stable+Diffusion_High-Resolution+Image+Synthesis+with+Latent+Diffusion+Models+ad2c2c9f-a13d-4f3e-be66-3d9a849d3bb7.md)

  * [Semi-Parametric Neural Image Synthesis](chapter3/article/Semi-Parametric+Neural+Image+Synthesis+9a82d68a-ee78-4a9c-ad37-f85b4289fde5.md)

  * [Stable Video Diffusion:Scaling Latent Video Diffusion Models to Large Datasets](chapter3/article/Stable+Video+Diffusion_Scaling+Latent+Video+Diffusion+Models+to+Large+Datasets+65b8898a-2004-4a60-9903-ccbc2ed5406a.md)

  * [DiT:Scalable Diffusion Models with Transformers](chapter3/article/DiT_Scalable+Diffusion+Models+with+Transformers+1d4f9206-91f7-4861-8782-7b1e0ff5f3f6.md)

  * [Sora:A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models](chapter3/article/Sora_A+Review+on+Background,+Technology,+Limitations,+and+Opportunities+of+Large+Vision+Models+0966ddb0-8aa3-40d8-970e-15989f3f10f1.md)

  * [Segment Anything](chapter3/article/Segment+Anything+2bc42de0-ad1f-4da2-80fb-8dae2a5a6998.md)

  * [ControlNet:Adding Conditional Control to Text-to-Image Diffusion Models](chapter3/article/ControlNet_Adding+Conditional+Control+to+Text-to-Image+Diffusion+Models+afd59b9e-8386-4b46-a613-47aa40528bb5.md)

  * [T2I-Adapter:Learning Adapters to Dig out More Controllable Ability for Text-to-Image Diffusion Models](chapter3/article/T2I-Adapter_Learning+Adapters+to+Dig+out+More+Controllable+Ability+for+Text-to-Image+Diffusion+Models+60861e5a-f727-4fe7-abd8-a6329cb9f71b.md)

  * [IP-Adapter:Text Compatible Image Prompt Adapter for Text-to-Image Diffusion Models](chapter3/article/IP-Adapter_Text+Compatible+Image+Prompt+Adapter+for+Text-to-Image+Diffusion+Models+a9162826-1d14-4514-9d2d-f198023bb5d4.md)

  * [Imagen:Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding](chapter3/article/Imagen_Photorealistic+Text-to-Image+Diffusion+Models+with+Deep+Language+Understanding+2f813578-9b75-462a-91db-3da3f94c5316.md)

  * [DALL·E:Zero-Shot Text-to-Image Generation](chapter3/article/DALLE_Zero-Shot+Text-to-Image+Generation+7984ab2f-4ff3-4f8b-879c-e56c3b2bb266.md)

  * [DALL·E2:unCLIP:Hierarchical Text-Conditional Image Generation with CLIP Latents](chapter3/article/DALLE2_unCLIP_Hierarchical+Text-Conditional+Image+Generation+with+CLIP+Latents+182d2279-99a1-4d86-87d6-b5841079e70a.md)

  * [DALL·E3:Improving Image Captioning with Better Use of Captions](chapter3/article/DALLE3_Improving+Image+Captioning+with+Better+Use+of+Captions+ebfb5ff7-60e1-48b4-82f2-f866a76df088.md)

  * [VQ-GAN:Taming Transformers for High-Resolution Image Synthesis](chapter3/article/VQ-GAN_Taming+Transformers+for+High-Resolution+Image+Synthesis+92f6b606-7163-4e0e-85f8-9733759bb5a1.md)

* [4. 强化学习](chapter4/README.md)
* [5. 大模型微调](chapter5/README.md)
  * [Adapter tuning:Parameter-Effificient Transfer Learning for NLP](chapter5/article/Adapter+tuning_Parameter-Effificient+Transfer+Learning+for+NLP+111ca3f7-74d2-4868-acdf-bc585d1cb8e5.md)

  * [Prefix-Tuning:Optimizing Continuous Prompts for Generation](chapter5/article/Prefix-Tuning_Optimizing+Continuous+Prompts+for+Generation+d8a5a2e2-14ea-43c8-a07d-0c0ab20392d4.md)

  * [Prompt Tuning:The Power of Scale for Parameter-Effificient Prompt Tuning](chapter5/article/Prompt+Tuning_The+Power+of+Scale+for+Parameter-Effificient+Prompt+Tuning+99f67c55-79e4-4d2c-99d6-888eea0520a3.md)

  * [P-Tuning:GPT Understands, Too](chapter5/article/P-Tuning_GPT+Understands,+Too+f7aae6c3-a284-4d19-8ed0-025631714281.md)

  * [P-Tuning v2:Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks](chapter5/article/P-Tuning+v2_Prompt+Tuning+Can+Be+Comparable+to+Fine-tuning+Universally+Across+Scales+and+Tasks+8f90aae4-c85c-4cdc-ac9c-e2c59af496dd.md)

  * [LoRA:Low-rank adaptation of large language models](chapter5/article/LoRA_Low-rank+adaptation+of+large+language+models+227bb108-d513-472a-9f8b-b7aa02c8ba97.md)

  * [IA3：Few-Shot Parameter-Effificient Fine-Tuning is Better and Cheaper than In-Context Learning](chapter5/article/IA3_Few-Shot+Parameter-Effificient+Fine-Tuning+is+Better+and+Cheaper+than+In-Context+Learning+8104afc8-80a0-49c2-b5c1-f8f6476bd9ad.md)


* [6. 参考文献](chapter6/README.md)
* [7. 贡献](chapter7/README.md)
  