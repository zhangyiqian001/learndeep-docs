在预训练后，LLM 可以获得解决各种任务的通用能力。然而，越来越多的研究表明，LLM 的能力可以进一步适配（adapting）到特定的目标。本节中，我们将介绍两种适配预训练后的 LLM 的方法：

- 指令微调（instruction tuning）：旨在增强（或解锁）LLM 的能力

- 对齐微调（alignment tuning）：旨在将 LLM 的行为与人类的价值观或偏好对齐。

## 指令微调

本质上，指令微调是在自然语言格式的实例（instance）集合上微调预训练后的 LLM 的方法 。这种方法与有监督微调 和多任务提示训练 密切相关。为了进行指令微调，我们首先需要收集或构建指令格式（instruction-formatted）的实例。然后，我们使用这种格式的实例以有监督的方式微调LLM（例如使用序列到序列的损失进行训练）。**指令微调后，LLM 可以展现出泛化到未见过任务的卓越能力，即使在多语言场景下也能有不错表现。**

通常情况下，一个指令格式的实例包括一个任务描述（称为指令）、一对输入-输出以及少量示例（可选）。数据集构建方法：

- 格式化已有数据集：使用人类撰写的任务描述来增广带标注的数据集，这些描述通过解释任务目标来指导LLM 理解任务。例如下图中(b)：每个问答任务的实例都添加了一个任务描述“请回答下列问题”。

- 格式化人类需求：InstructGPT 建议采用真实用户提交给 OpenAI API 的查询作为任务描述。

![image.png](大模型微调+6321a235-4d5c-4243-a77a-ef715fee2d37/image.png)

## 对齐微调

LLM 在多个自然语言处理任务上展示出了惊人的能力。但是, 这些模型有时可能表现出预期之外的行为，例如编造虚假信息、追求不准确的目标，以及产生有害的、误导性的和有偏见的表达。**对齐微调需要考虑的标准（例如有用性, 诚实性和无害性），已有研究表明对齐微调可能会在某种程度上损害 LLM 的通用能力。**

- 有用性： LLM 为了达到有用性，应当尽其所能以简明扼要且高效的方式帮助用户解决任务或回答问题。

- 诚实性：在基本层面上，诚实的 LLM 应该向用户提供准确的内容，而不会捏造信息。

- 无害性：无害性要求模型生成的语言不得是冒犯性或歧视性的。

主要实现方法：RLHF

## 高效微调

在上文中，我们讨论了指令微调和对齐微调的方法，以使 LLM适应特定的目标。由于 LLM 包含大量的模型参数，进行全参数微调将会有较大开销。在本节中，我们将讨论如何对 LLM进行高效微调。

包括适配器微调（adapter tuning）、前缀微调（prefix tuning）、提示微调（prompt tuning）和低秩适配（LoRA）

### 适配器微调（adapter tuning）

**适配器微调在 Transformer 模型中引入了小型神经网络模块（称为适配器）** 。为了实现适配器模块，在中提出了一个瓶颈架构，它首先将原始特征向量压缩到较小的维度（然后进行非线性变换），然后将其恢复到原始维度。适配器模块将被集成到每个 Transformer 层中，通常使用串行插入的方式，分别在 Transformer 层的两个核心部分（即注意力层和前馈层）之后。另外，在 Transformer 层中也可以使用并行适配器 ，其将两个适配器模块与注意力层和前馈层并行放置。在微调过程中，适配器模块将根据特定的任务目标进行优化，而原始语言模型的参数将在此过程中保持不变。通过这种方式，我们可以在微调过程中有效地减少可训练参数的数量。

### 前缀微调（prefix tuning）

**前缀微调 在语言模型的每个 Transformer 层前添加了一系列前缀，这些前缀是一组可训练的连续向量。**这些前缀向量具有任务的特异性，可以视为虚拟的 token 嵌入。为了优化前缀向量，文章 [229] 提出了一种重参数化技巧，即学习一个将较小矩阵映射到前缀参数矩阵的 MLP 函数，而不是直接优化前缀。经证明，该技巧对于稳定训练很有帮助。优化后，映射函数将被舍弃，只保留派生的前缀向量以增强与特定任务相关的性能。由于只有前缀参数会被训练，因此可以实现参数高效的模型优化。类似于前缀微调，p-tuning v2 [235]特别为自然语言理解而在 Transformer 架构中引入了逐层提示向量，并且还利用多任务学习来联合优化共享的提示。已经证明，它能有效提高不同参数规模的模型在自然语言理解任务上的性能。

### 提示微调（prompt tuning）

提示微调：与前缀微调不同，提示微调 [230, 236] 主要是在输入层中加入可训练的提示向量24。基于离散提示方法 [238, 239]，它通过包含一组软提示 token（以自由形式 [236] 或前缀形式 [230]）来扩充输入文本，然后将扩充后的输入用于解决特定的下游任务。在实现中，任务特定的提示嵌入与输入文本嵌入相结合，然后输入到语言模型中。P-tuning [236] 提出了一种自由形式来组合上下文、提示和目标 token，适用于自然语言理解和生成的架构。他们还通过双向 LSTM 学习了软提示 token 的表示。另一种称为提示微调的代表性方法 [230] ，直接在输入前加入前缀提示。在训练过程中，只有提示嵌入会根据特定任务的监督进行学习。然而，由于该方法在输入层只包含少量可训练参数，已发现其性能高度依赖底层语言模型的能力。

### 低秩适配（LoRA）

低秩适配（LoRA） [231] 通过添加低秩约束来近似每层的更新矩阵，以减少适配下游任务的可训练参数。考虑优化参数矩阵 W 的情况。更新过程可以写成一般形式：$W ←W + ∆W$。LoRA 的基本思想是冻结原始矩阵$ W ∈ R^{m×n}$，同时通过低秩分解矩阵来近似参数更新矩阵$ ∆W = A · B ^T$，其中 $A ∈ R^{m×k }$和 $B ∈ R^{n×k} $是用于任务适配的可训练参数，$r ≪ min(m, n)$ 是降低后的秩。LoRA 的主要优点是可以大大节省内存和存储使用（例如 VRAM）。此外，人们可以只保留一个大型模型副本，同时保留多个用于适配不同下游任务的特定低秩分解矩阵。此外，还有几项研究讨论了如何以更有原则的方法设置秩，例如基于重要性分数的分配 和无需搜索的最优秩选择。
