# 目录

* [Playing Atari with Deep Reinforcement Learning](article/Playing+Atari+with+Deep+Reinforcement+Learning+827583df-e794-463c-99f2-cf3f67fe9990.md)

* [FCN:Fully Convolutional Networks for Semantic Segmentation](article/FCN_Fully+Convolutional+Networks+for+Semantic+Segmentation+7ba6b006-838f-409c-916e-0699de48b3fc.md)

* [U-Net:Convolutional Networks for Biomedical Image Segmentation](article/U-Net_Convolutional+Networks+for+Biomedical+Image+Segmentation+abc51257-7672-432e-83ce-d920552d8550.md)

* [GAN:Generative Adversarial Nets](article/GAN_Generative+Adversarial+Nets+fc1b0774-6173-4f34-b424-dbab8a5d3af7.md)

* [Attention Is All You Need](article/Attention+Is+All+You+Need+5697d7af-2155-4eb4-9e3a-63d7727c1b17.md)

* [GPT-1:Improving Language Understanding by Generative Pre-Training](article/GPT-1_Improving+Language+Understanding+by+Generative+Pre-Training+e8311bc0-b181-4b42-91c5-6ec0eabf3bb1.md)

* [InstructGPT:Training language models to follow instructions with human feedback](article/InstructGPT_Training+language+models+to+follow+instructions+with+human+feedback+295ddc33-d3d8-401b-aaf6-e712f063609c.md)

* [BERT:Pre-training of Deep Bidirectional Transformers for Language Understanding](article/BERT_Pre-training+of+Deep+Bidirectional+Transformers+for+Language+Understanding+c63ffe59-577a-4a8e-812b-a306ea91f8d8.md)

* [BART:Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](article/BART_Denoising+Sequence-to-Sequence+Pre-training+for+Natural+Language+Generation,+Translation,+and+Comprehension+da7bcf69-e913-402c-9506-16221b816a95.md)

* [T5:Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](article/T5_Exploring+the+Limits+of+Transfer+Learning+with+a+Unified+Text-to-Text+Transformer+f135b4cd-6d37-4a2f-a579-90c5736378f0.md)

* [ELMo:Deep contextualized word representations](article/ELMo_Deep+contextualized+word+representations+28adb18f-986e-4525-bcda-35c5d9175dcd.md)

* [ViT:AN IMAGE IS WORTH 16X16 WORDS_ TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE](article/ViT_AN+IMAGE+IS+WORTH+16X16+WORDS_+TRANSFORMERS+FOR+IMAGE+RECOGNITION+AT+SCALE+4f160327-28cf-4943-9f52-b36de432337e.md)

* [Distilling the Knowledge in a Neural Network](article/Distilling+the+Knowledge+in+a+Neural+Network+a4d51fb7-586f-4bee-a293-f6bcfb41c843.md)

* [DeiT:Training data-efficient image transformers & distillation through attention](article/DeiT_Training+data-efficient+image+transformers+++distillation+through+attention+84d04d5c-2835-45c9-8545-9e007920f9ba.md)

* [Swin Transformer:Hierarchical Vision Transformer using Shifted Windows](article/Swin+Transformer_Hierarchical+Vision+Transformer+using+Shifted+Windows+9d8ba80e-b366-4464-9187-7de59aa85760.md)

* [DETR:End-to-End Object Detection with Transformers](article/DETR_End-to-End+Object+Detection+with+Transformers+bb4f3503-1f59-4e34-b261-3b83165e6e9e.md)

* [CLIP:Learning Transferable Visual Models From Natural Language Supervision](article/CLIP_Learning+Transferable+Visual+Models+From+Natural+Language+Supervision+9efc2900-5160-4a05-a624-68a35d26c23f.md)

* [VAE:Auto-Encoding Variational Bayes](article/VAE%EF%BC%9AAuto-Encoding+Variational+Bayes+ff3dd745-4822-4bb1-9c99-d931512df77b.md)

* [VQ-VAE:Neural Discrete Representation Learning](article/VQ-VAE_Neural+Discrete+Representation+Learning+207156ff-8f12-497e-a0f5-b11fdf2b3712.md)

* [VQ-VAE2:Generating Diverse High-Fidelity Images with VQ-VAE-2](article/VQ-VAE2_Generating+Diverse+High-Fidelity+Images+with+VQ-VAE-2+d8d21676-11d4-4836-98d2-9babbe65e784.md)

* [KAN:Kolmogorov–Arnold Networks](article/KAN_Kolmogorov%E2%80%93Arnold+Networks+a7e663f0-8c8c-4071-8630-eacaf17a50f4.md)

* [Pixel RNN:Pixel Recurrent Neural Networks](article/Pixel+RNN_Pixel+Recurrent+Neural+Networks+df0af303-e9af-4a29-a74d-2696b9a3c36e.md)

* [Conditional Image Generation with PixelCNN Decoders](article/Conditional+Image+Generation+with+PixelCNN+Decoders+607b537c-9732-4579-b29c-8812a7dbcef2.md)

* [GQA:Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints](article/GQA_Training+Generalized+Multi-Query+Transformer+Models+from+Multi-Head+Checkpoints+9c3cb968-6f34-4e84-8602-98246c47efe1.md)

* [FlashAttention:Fast and Memory-Efficient Exact Attention with IO-Awareness](article/FlashAttention_Fast+and+Memory-Efficient+Exact+Attention+with+IO-Awareness+93390a33-b714-4954-afed-1930b535c3a0.md)

* [Efficient Memory Management for Large Language Model Serving with PagedAttention](article/Efficient+Memory+Management+for+Large+Language+Model+Serving+with+PagedAttention+984c0dc0-250b-4618-abb2-41a85f68c588.md)