# 目录

* [Playing Atari with Deep Reinforcement Learning](article/Playing+Atari+with+Deep+Reinforcement+Learning.md)

* [FCN:Fully Convolutional Networks for Semantic Segmentation](article/Fully+Convolutional+Networks+for+Semantic+Segmentation.md)

* [U-Net:Convolutional Networks for Biomedical Image Segmentation](article/Convolutional+Networks+for+Biomedical+Image+Segmentation.md)

* [GAN:Generative Adversarial Nets](article/Generative+Adversarial+Nets.md)

* [Attention Is All You Need](article/Attention+Is+All+You+Need.md)

* [GPT-1:Improving Language Understanding by Generative Pre-Training](article/Improving+Language+Understanding+by+Generative+Pre-Training.md)

* [InstructGPT:Training language models to follow instructions with human feedback](article/Training+language+models+to+follow+instructions+with+human+feedback.md)

* [BERT:Pre-training of Deep Bidirectional Transformers for Language Understanding](article/Pre-training+of+Deep+Bidirectional+Transformers+for+Language+Understanding.md)

* [BART:Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](article/Denoising+Sequence-to-Sequence+Pre-training+for+Natural+Language+Generation,+Translation,+and+Comprehension.md)

* [T5:Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](article/Exploring+the+Limits+of+Transfer+Learning+with+a+Unified+Text-to-Text+Transformer.md)

* [ELMo:Deep contextualized word representations](article/Deep+contextualized+word+representations.md)

* [ViT:AN IMAGE IS WORTH 16X16 WORDS:TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE](article/AN+IMAGE+IS+WORTH+16X16+WORDS+TRANSFORMERS+FOR+IMAGE+RECOGNITION+AT+SCALE.md)

* [Distilling the Knowledge in a Neural Network](article/Distilling+the+Knowledge+in+a+Neural+Network.md)

* [DeiT:Training data-efficient image transformers & distillation through attention](article/Training+data-efficient+image+transformers+++distillation+through+attention.md)

* [Swin Transformer:Hierarchical Vision Transformer using Shifted Windows](article/Hierarchical+Vision+Transformer+using+Shifted+Windows.md)

* [DETR:End-to-End Object Detection with Transformers](article/End-to-End+Object+Detection+with+Transformers.md)

* [CLIP:Learning Transferable Visual Models From Natural Language Supervision](article/Learning+Transferable+Visual+Models+From+Natural+Language+Supervision.md)

* [VAE:Auto-Encoding Variational Bayes](article/Auto-Encoding+Variational+Bayes.md)

* [VQ-VAE:Neural Discrete Representation Learning](article/Neural+Discrete+Representation+Learning.md)

* [VQ-VAE2:Generating Diverse High-Fidelity Images with VQ-VAE-2](article/Generating+Diverse+High-Fidelity+Images+with+VQ-VAE-2.md)

* [KAN:Kolmogorov–Arnold Networks](article/Kolmogorov-Arnold+Networks.md)

* [Pixel RNN:Pixel Recurrent Neural Networks](article/Pixel+Recurrent+Neural+Networks.md)

* [Conditional Image Generation with PixelCNN Decoders](article/Conditional+Image+Generation+with+PixelCNN+Decoders.md)

* [GQA:Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints](article/Training+Generalized+Multi-Query+Transformer+Models+from+Multi-Head+Checkpoints.md)

* [FlashAttention:Fast and Memory-Efficient Exact Attention with IO-Awareness](article/Fast+and+Memory-Efficient+Exact+Attention+with+IO-Awareness.md)

* [Efficient Memory Management for Large Language Model Serving with PagedAttention](article/Efficient+Memory+Management+for+Large+Language+Model+Serving+with+PagedAttention.md)