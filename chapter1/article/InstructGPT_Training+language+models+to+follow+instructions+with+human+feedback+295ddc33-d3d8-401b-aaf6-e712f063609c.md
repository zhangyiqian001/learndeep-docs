2203.02155v1

# 摘要

使语言模型更大本身并不能使它们更好地遵循用户的意图。例如，大型语言模型可以生成不真实、有毒或对用户根本没有帮助的输出。在本文中，我们展示了一种通过对人类反馈进行微调，在广泛的任务上调整语言模型的方法。从一组通过OpenAI API提交的标签编写的提示和提示开始，我们收集了所需模型行为的标签演示数据集，我们使用监督学习对GPT-3进行微调。然后，我们收集了一个模型输出的排名数据集，我们使用这些数据集，使用来自人类反馈的强化学习来进一步微调这个有监督的模型。我们称结果模型为InstructGPT。在我们对提示分布的人工评估中，1.3B参数InstructGPT模型的输出优于175B GPT-3的输出，尽管参数少了100倍。此外，InstructGPT模型显示了真实性的改善和毒性产量的减少，同时在公共NLP数据集上有最小的性能回归。尽管InstructGPT仍然犯简单的错误，我们的结果表明，使用人类反馈进行微调是调整语言模型的一个有前途的方向。

## 导言

![image.png](InstructGPT:Training+language+models+to+follow+instructions+with+human+feedback+295ddc33-d3d8-401b-aaf6-e712f063609c/image.png)

图1：在我们的API提示分布上对各种模型的人工评估，通过每个模型的输出比175B SFT模型的首选频率进行评估。我们的InstructGPT模型（PPO-ptx）及其未经预训练混合训练（PPO）显著优于GPT-3基线（GPT，GPT prompted）；我们的1.3B PPO-ptx模型的输出比175B GPT-3的输出更受青睐。整个论文的误差条是95%的置信区间。

我们专注于对齐语言模型的微调方法。具体来说，我们使用来自人类反馈的强化学习（RLHF）来对GPT-3进行微调，以遵循广泛的书面指令类别。这种技术利用人类的偏好作为奖励信号来微调我们的模型。

- 我们首先雇佣了一个由40名承包商组成的团队，根据他们在筛选测试中的表现，对我们的数据进行标签化。

- 然后，我们收集了提交到OpenAI API的提示（主要是英语）的人工编写的演示数据集，并使用它来训练我们的监督学习基线。

- 接下来，我们在更大的API提示集上收集来自我们的模型的输出之间的人工标记比较数据集。

- 然后，我们在这个数据集上训练一个奖励模型（RM），以预测我们的标签人员更喜欢哪个模型输出。

- 最后，我们使用这个RM作为奖励函数，并使用PPO算法微调我们的监督学习基线，以最大化这个奖励。

我们将在图2中举例说明这个过程。这一过程将GPT-3的行为与特定群体（主要是我们的劳工和研究人员）的偏好相一致，而不是任何更广泛的“人类价值观”

## 方法

1. 第一步收集示范数据，并培训一个有监督的策略。我们的标签符在输入提示分布上提供了所需行为的演示。然后，我们使用监督学习方法对该数据进行了预先训练过的GPT-3模型的微调。

2. 第二步：收集比较数据，并训练一个奖励模型。我们收集模型输出之间的比较数据集，其中标记符表示它们更喜欢给定输入的哪个输出。然后，我们训练一个奖励模型来预测人类偏好的输出。

3. 第三步：使用PPO针对奖励模型优化策略。我们使用RM的输出作为标量奖励。我们使用PPO算法对监督策略进行微调，以优化该奖励。

步骤2和步骤3可以连续迭代；根据当前的最佳策略收集了更多的比较数据，用于训练一个新的RM，然后再训练一个新的策略。实际上，我们的大部分比较数据来自我们的监督政策，其中一些来自我们的PPO政策。

![image.png](InstructGPT:Training+language+models+to+follow+instructions+with+human+feedback+295ddc33-d3d8-401b-aaf6-e712f063609c/image 1.png)

图说明了我们的方法的三个步骤：

1. supervised fifine-tuning（SFT，监督微调）：人提问题，人做回答，微调模型。

2. reward model (RM，奖励模型)训练：人提问题，模型生成答案，人标注答案好坏顺序作为输入样本，训练奖励模型。

3. 通过proximal policy optimization（PPO，近端策略优化）进行强化学习：找一个新的样本，PPO策略生成一个输出，奖励模型计算输出奖励，奖励用于使用PPO更新策略。

蓝色箭头表示这些数据被用于训练我们的一个模型。在步骤2中，盒子A-D是来自我们的模型的样本，按标签符进行排序。

## 模型

- 有监督的微调（SFT）。我们使用监督学习对标签演示中的GPT-3进行了微调。我们训练了16个epochs，使用余弦学习率衰减，residual dropout of 0.2。我们根据验证集上的RM分数进行最终的SFT模型选择。与Wu等人（2021）类似，我们发现我们的SFT模型对1个epochs后的验证损失进行了过拟合；然而，我们发现，对更多epochs的训练可以帮助RM评分和人类偏好评级。

- 奖励建模（RM）。从去掉最终的解嵌入层的SFT模型开始，我们训练了一个模型来接受一个提示和响应，并输出一个标量奖励。在本文中，我们只使用6B RMs，因为这节省了大量的计算，我们发现175B RM训练可能是不稳定的，因此不太适合作为RL期间的值函数。

- 强化学习（RL）。继斯蒂恩农等人（2020年）之后，我们再次使用PPO对我们的环境中的SFT模型进行了微调。环境是一个强盗环境，它随机显示客户提示，并期望对提示的响应。给定提示和反应，它会产生一个由奖励模型决定的奖励，并结束这个情节。此外，我们在每个标记上添加了一个来自SFT模型的每个标记KL惩罚，以减轻奖励模型的过度优化。值函数从RM值初始化。我们称这些模型为“PPO”。

