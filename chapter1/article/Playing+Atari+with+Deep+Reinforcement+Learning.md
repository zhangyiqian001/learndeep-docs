# 摘要

本文提出首个能通过强化学习，直接从高维感官输入中成功学习控制策略的深度学习模型。该模型为卷积神经网络，采用 Q 学习变体进行训练，以原始像素作为输入，输出用于估计未来奖励的价值函数。将此方法应用于街机学习环境中的 7 款雅达利 2600 游戏，无需调整架构或学习算法。结果表明，该模型在 6 款游戏中超越了以往所有方法，在 3 款游戏中超过了人类专家水平。

## 方法

### 深度 Q 学习算法

在强化学习领域，传统的在线学习方式在处理高维数据与复杂环境时存在一定局限。本文提出的深度 Q 学习算法，创新性地引入了经验回放机制，极大地提升了学习效率与稳定性。

**经验存储**：智能体在与环境（雅达利游戏模拟器）交互的过程中，每个时间步的经验$$e_{t}=(s_{t}, a_{t}, r_{t}, s_{t + 1})$$都会被存储到回放记忆$$D = {e_1, ..., e_N}$$中。这里的$$s_{t}$$代表智能体在$$t$$时刻观察到的状态，由一系列的动作和观察结果组成；$$a_{t}$$是智能体在$$t$$时刻选择的动作；$$r_{t}$$为执行动作$$a_{t}$$后获得的奖励；$$s_{t + 1}$$则是执行动作后转移到的新状态。这种存储方式打破了数据的时间相关性，为后续的学习提供了丰富多样的样本。

**学习更新**：在算法的内循环中，从回放记忆$$D$$中随机抽取小批量的经验样本$$(\varphi_j, a_j, r_j, \varphi_{j + 1})$$。对于每个样本，计算目标值$$y_j$$：若$$\varphi_{j + 1}$$为非终止状态，$$y_j = r_j + \gamma \max_{a'} Q(\varphi_{j + 1}, a'; \theta)$$；若$$\varphi_{j + 1}$$为终止状态，$$y_j = r_j$$。其中，$$\gamma$$为折扣因子，用于平衡即时奖励与未来奖励，$$Q(\varphi, a; \theta)$$是以$$\theta$$为参数的动作价值函数。随后，通过随机梯度下降法，对损失函数$$(y_j - Q(\varphi_j, a_j; \theta))^2$$进行梯度下降更新，调整 Q 网络的参数$$\theta$$。

**动作选择**：智能体依据$$\epsilon$$- 贪婪策略选择并执行动作。以$$1 - \epsilon$$的概率选择使$$Q$$值最大的动作$$a = \max_a Q(\varphi(s_t), a; \theta)$$，以探索当前认为的最优策略；以$$\epsilon$$的概率随机选择动作，保证对状态空间的充分探索。在训练过程中，$$\epsilon$$通常会随着时间退火，从初始值逐渐减小，使得智能体在训练初期能够广泛探索环境，后期则更倾向于利用已学习到的最优策略。

### 预处理和模型架构

**数据预处理**：原始的雅达利游戏画面为 210×160 像素的 RGB 图像，拥有 128 种颜色调色板，直接处理这些数据对计算资源的需求极高。因此，本文采用了一系列的预处理步骤来降低输入维度。首先，将 RGB 图像转换为灰度图，减少颜色信息，降低数据量；接着，将图像下采样至 110×84 大小，进一步降低数据维度；最后，裁剪出 84×84 的区域，以适配 GPU 实现的 2D 卷积操作对输入为正方形的要求。此外，算法 1 中的函数$$\varphi$$会将最后 4 帧经过预处理的图像堆叠起来，形成 84×84×4 的输入，为模型提供更丰富的时间序列信息。

**网络架构设计**：为了高效地计算 Q 值，本文设计了一种特殊的网络架构。该架构的输入为 84×84×4 的图像，首先经过两个卷积层。第一层卷积层使用 16 个 8×8 的滤波器，步长为 4，并应用整流非线性激活函数 \[10, 18]，用于提取图像的低级特征；第二层卷积层使用 32 个 4×4 的滤波器，步长为 2，同样接整流非线性激活函数，进一步提取高级特征。随后，连接一个包含 256 个整流单元的全连接隐藏层，对提取到的特征进行整合。最后，输出层为全连接线性层，针对每个有效的游戏动作（不同游戏的有效动作数量在 4 到 18 之间）都有一个单独的输出，这些输出值即为该动作在当前输入状态下的预测 Q 值。这种架构使得模型能够通过一次前向传播，计算出所有可能动作的 Q 值，大大提高了计算效率。

## 实验

**实验设置**：在 7 款雅达利游戏上进行实验，采用相同的网络架构、学习算法和超参数设置。训练时对奖励结构进行调整，将所有正奖励设为 1，负奖励设为 - 1。使用 RMSProp 算法，小批量大小为 32，ε 从 1 线性退火至 0.1，训练 1000 万帧，回放记忆存储 100 万帧。除《太空侵略者》采用帧跳过技术$$k = 3$$外，其他游戏$$k = 4$$。

**实验结果**：训练过程中，平均预测 Q 值平稳提升，验证了方法的稳定性。可视化价值函数表明，模型能学习复杂事件序列中价值函数的演变。与其他方法对比，本文的深度 Q 网络（DQN）在 7 款游戏中的 6 款上取得了领先成绩，在《打砖块》《耐力赛》和《乒乓》游戏中超越了人类专家水平，在《光束骑手》游戏中接近人类表现。

## 相关工作

**TD - gammon**：通过强化学习和自我对弈学习，在双陆棋游戏中达到超人水平，采用类似 Q 学习的无模型强化学习算法，用多层感知机近似价值函数。

**神经拟合 Q 学习（NFQ）**：优化损失函数序列以更新 Q 网络参数，采用批量更新，计算成本与数据集大小成正比。而本文采用随机梯度更新，计算成本低，可扩展到大型数据集。

**其他方法**：此前有研究将深度学习与强化学习结合，使用深度神经网络估计环境、受限玻尔兹曼机估计价值函数或策略；也有研究将 Q 学习与经验回放和简单神经网络结合，但这些方法大多基于低维状态，而非原始视觉输入。

## QA

### 问：论文提出的方法与传统强化学习方法有何不同？

答：传统强化学习方法多依赖手工设计的特征与线性价值函数或策略表示，且在处理高维感官输入时面临诸多挑战。本文方法直接从原始像素输入学习，通过卷积神经网络提取特征，并结合经验回放机制和 Q 学习变体训练网络，能够克服高维感官数据带来的挑战，在复杂强化学习环境中学习成功的控制策略。

### 问：经验回放机制有什么优势？

答：经验回放机制有三方面优势。一是提高数据效率，每个经验步骤可用于多次权重更新；二是打破连续样本间的强相关性，降低更新的方差；三是避免因当前参数决定下一个训练数据样本而导致的反馈循环，使行为分布在多个先前状态上平均，避免参数陷入局部最小值或发散。

## 引用

[Playing Atari with Deep Reinforcement Learning](https://arxiv.org/abs/1312.5602)
