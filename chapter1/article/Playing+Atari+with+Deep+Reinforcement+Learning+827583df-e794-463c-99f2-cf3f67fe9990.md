1312.5602v1

强化学习开山篇

# 摘要

我们提出了第一个深度学习模型，成功地学习直接从高维感官输入的控制策略使用强化学习。该模型是一个卷积神经网络，用q学习的一个变体进行训练，其输入是原始像素，其输出是一个估计未来奖励的值函数。我们将我们的方法应用于来自街机学习环境中的7款雅达利2600游戏，而没有调整架构或学习算法。我们发现它在六款游戏上优于之前的所有方法，在其中三款游戏上也超过了人类专家。

## 导言

学习直接从视觉和语音等高维感官输入中控制主体，是强化学习（RL）长期以来面临的挑战之一。在这些域上运行的大多数成功的RL应用程序都依赖于手工制作的特性，并结合了线性值函数或策略表示。显然，这类系统的性能在很大程度上依赖于特征表示的质量。

深度学习的最新进展使得从原始感官数据中提取高级特征成为可能，从而在计算机视觉[11,22,16]和语音识别[6,7]方面取得了突破。这些方法利用了一系列的神经网络结构，包括卷积网络、多层感知器、限制性玻尔兹曼机和递归神经网络，并利用了有监督和无监督学习。似乎很自然地问，类似的技术是否也对感觉数据的RL有益。

然而，从深度学习的角度来看，强化学习提出了一些挑战。首先，迄今为止，大多数成功的深度学习应用程序都需要大量的手标记训练数据。另一方面，RL算法必须能够从标量奖励信号稀疏、噪声和延迟的标量奖励信号中学习。与在监督学习中发现的输入和目标之间的直接关联相比，行动和结果奖励之间的延迟可能需要数千个时间步长，这似乎特别令人生畏。另一个问题是，大多数深度学习算法假设数据样本是独立的，而在强化学习中，人们通常会遇到高度相关的状态序列。此外，在RL中，数据分布随着算法学习新的行为而发生变化，这对于假设有一个固定的底层分布的深度学习方法来说可能是有问题的。

