2308.06721v1

# 摘要

近年来，我们已经见证了大型文本到图像扩散模型的强大力量，它具有创建高保真图像的生成能力。然而，仅使用文本提示符来生成所需的图像是非常棘手的，因为它通常涉及到复杂的提示符工程。文本提示的另一种方法是图像提示，俗话说：“一个图像值千言万语”。尽管现有的从预训练过的模型中进行直接微调的方法是有效的，但它们需要大量的计算资源，并且与其他基本模型、文本提示和结构控制不兼容。**在本文中，我们提出了IP-Adapter，一种有效的和轻量级的适配器，以实现预训练的文本到图像扩散模型的图像提示能力。我们的ip适配器的关键设计是解耦的交叉注意机制，它分离了文本特征和图像特征的交叉注意层。**尽管我们的方法很简单，但一个只有22M参数的IP-Adapter可以实现与完全微调的图像提示模型相当甚至更好的性能。当我们冻结预先训练的扩散模型时，所提出的IP-Adapter不仅可以推广到其他从同一基模型进行微调的其他自定义模型，而且还可以推广到使用现有的可控工具进行可控生成。利用解耦的交叉注意策略，图像提示也可以很好地配合文本提示，从而实现多模态图像的生成。

## 导言

随着最近的大型文本到图像扩散模型的成功，图像生成已经取得了显著的进展，如GLIDE [1], DALL-E 2 [2], Imagen [3], Stable Diffusion (SD) [4], eDiff-I [5] and RAPHAEL [6].。用户可以编写文本提示生成强大的文本到图像扩散模型。但是编写好的文本提示来生成所需的内容并不容易，因为通常需要复杂的提示工程[7]。此外，文本并不能提供信息来表达复杂的场景或概念，这可能会阻碍内容的创建。考虑到文本提示的上述限制，我们可能会询问是否有其他提示类型来生成图像。一个很自然的选择是使用图像提示符，因为与文本相比，一个图像可以表达更多的内容和细节，就像人们经常说的那样：“一个图像值千言万语”。DALL-E 2[2]首次尝试支持图像提示，扩散模型以图像嵌入而不是文本嵌入为条件，需要一个预先的模型来实现文本到图像的能力。然而，大多数现有的文本到图像的扩散模型都是基于文本来生成图像的，例如，流行的SD模型是基于从冻结的CLIP [8]文本编码器中提取的文本特征。这些文本到图像的扩散模型也支持图像提示吗？我们的工作试图以一种简单的方式实现图像提示对这些文本到图像扩散模型的生成能力。

之前的工作，如SD图像变化和Stable unCLIP，已经证明了直接对图像嵌入的文本条件扩散模型进行微调以实现图像提示功能的有效性。然而，这种方法的缺点是很明显的。首先，它消除了使用文本生成图像的原始能力，并且这种微调通常需要大量的计算资源。其次，微调后的模型通常不可重用，因为图像提示能力不能直接转移到来自相同的文本到图像基础模型的其他自定义模型。此外，新模型通常与现有的结构控制工具如ControlNet [9]不兼容，这给下游应用程序带来了重大挑战。由于微调的缺点，一些研究表明，[10]选择用图像编码器代替文本编码器，同时避免对扩散模型进行微调。虽然这种方法有效、简单，但仍存在一些缺点。首先，只支持图像提示，防止用户同时使用文本和图像提示来创建图像。此外，仅仅对图像编码器进行微调往往不足以保证图像质量，并可能导致泛化问题。

在本研究中，我们很好奇是否有可能在不修改原始文本到图像模型的情况下实现图像提示能力。幸运的是，之前的工作都很令人鼓舞。可控图像生成的最新进展，如s ControlNet [9] and T2I-adapter，已经证明了一个额外的网络可以有效地插入现有的文本到图像扩散模型来指导图像生成。大多数的研究都集中在附加结构控制的图像生成上，如用户绘制的草图、深度图、语义分割图等。此外，通过简单的适配器，如T2I适配器[11]的样式适配器和单控制网[12]的全局控制器，也实现了由参考图像提供的样式或内容的图像生成。为了实现这一点，从CLIP图像编码器中提取的图像特征通过一个可训练的网络映射到新的特征上，然后与文本特征进行连接。通过替换原始文本特征，将合并后的特征输入扩散模型的UNet，以指导图像的生成。这些适配器可以被看作是一种能够使用图像提示符的方式，但这些属



